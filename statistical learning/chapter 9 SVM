i#nstall.packages('e1071')
library(e1071)
set.seed(1)
x<-matrix(rnorm(20*2),ncol=2)
y<-c(rep(-1,10),rep(1,10))
y
x[y==1,]<-x[y==1,]+1
x[y==1,]
plot(x,col=(3-y)) #to make 2 different colors using col=(3-y)

#############################################################################################
##Linear SVM: support vector classifier 
##svm(kernel='linear')
##A very important parameter is cost, which is the tuning parameter. The larger the tuning 
##parameter,the smaller margins, the less support vector.
##Tuning parameter 和margin是反比关系。
#############################################################################################

dat<-data.frame(x=x,y=as.factor(y))
svmfit<-svm(y~.,data=dat,kernel='linear',cost=10,scale = F)
plot(svmfit,dat)
svmfit$index #gives the indexes for the support vectors.
summary(svmfit)
?svm()

#to use a smaller value for the cost parameter.
svmfit1<-svm(y~.,data=dat,kernel='linear',cost=0.1,scale = F)
plot(svmfit1,dat)
svmfit1$index #smaller value for the cost parameter, larger number of support vectors.
set.seed(1)
tune.out<-tune(svm,y~.,data=dat,kernel='linear',ranges = list(cost=c(0.001,0.01,0.1,1,5,10,100)))
summary(tune.out) #it directly give the best parameter of cost(因为相应的error值最小)
bestmod<-tune.out$best.model
summary(bestmod)

#make a prediction 
xtest<-matrix(rnorm(20*2),ncol=2)
ytest<-sample(c(-1,1),20,rep=T)
ytest
xtest[ytest==1,]<-xtest[ytest==1,]+1
testdat<-data.frame(x=xtest,y=as.factor(ytest))
ypred<-predict(bestmod,newdata = testdat)
table(predict=ypred,truth=testdat$y)

#a large cost,smaller margins, fewer support vectors. 
x[y==1,]<-x[y==1,]+0.5
plot(x,col=(y+5)/2,pch=19)
dat<-data.frame(x=x,y=as.factor(y))
svmfit<-svm(y~.,data=dat,kernel='linear',cost=1e5)
summary(svmfit)
plot(svmfit,dat)

#a smaller cost, larger margin, more support vectors.
svmfit1<-svm(y~.,data=dat,kernel='linear',cost=1)
summary(svmfit1)
plot(svmfit1,dat)


###############################################################################################
##Support Vector Machine
##SVM is used for non-linear kernel,
##polynomial kernel: kernel='polynomial'
##radial kernel: kernel='radial'

set.seed(1)
x<-matrix(rnorm(200*2),ncol=2)
x[1:100,]<-x[1:100,]+2
x[101:150,]<-x[101:150]-2
y<-c(rep(1,150),rep(2,50))
dat<-data.frame(x=x,y=as.factor(y))
plot(x,col=y) #it shows that the two colors are not easy to be seperated.

#to split the training data and test data.
sid<-sample(200,100) #从200个中取100个数。
train<-dat[sid,]
test<-dat[-sid,]
svmfit<-svm(y~.,data=train,kernel='radial',gamma=1,cost=1)
plot(svmfit,train) #finds a non-linear decision boundary.
summary(svmfit)

#increase the cost, smaller margin, smaller support vectors,reduce the training error.However, may overfit.
svmfit<-svm(y~.,data=train,kernel='radial',cost=1e5)
plot(svmfit,train)

##Use cross-validataion to tune the best choice of parameter.
set.seed(1)
tune.out<-tune(svm,y~.,data=train,kernel='radial',ranges = list(cost=c(0.1,1,10,100,1000)),gamma=c(0.5,1,2,3,4))
summary(tune.out) #参数最好的选择：cost=1
table(true=test[,'y'],pred=predict(tune.out$best.model,newx=test))



#############################################################################################
##ROC Curve 
##make a function to draw roc plot
#################################################################################################


#install.packages('rocr')
library('ROCR')

#ROC curve
rocplot<-function(pred,truth,...){
  predob<-prediction(pred,truth) #pred<-prediction(target_perd,target_class)
  perf<-performance(predob,'tpr','fpr') #perf<-performance(pred,'tpr','fpr') | AUC<-performance(pred,'auc')
  plot(perf,...)
}

svmfit.opt<-svm(y~.,data = train,kernel='radial',gamma=2,cost=1,decision.value=T)
fitted<-attributes(predict(svmfit.opt,newdata = train,decision.values = T))$decision.values
train
par(mfrow=c(1,2))
rocplot(fitted,dat[sid,'y'],main='Training Data')


##############################################################################################
##SVM with multiple classes
################################################################################################
set.seed(1)
x<-rbind(x,matrix(rnorm(50*2),ncol=2))
y<-c(y,rep(0,50)) #在原来的x,y的基础上多加了50 row,并且多了一类
nrow(x)
length(y)
x[y==0,2]<-x[y==0,2]+2
y
dat<-data.frame(x=x,y=as.factor(y))
par(mfrow=c(1,1))
plot(x,col=(y+1)) #three colors,说明有三类要进行区分。

#fit the SVM
svmfit<-svm(y~.,data = dat,kernel='radial',cost=10,gamma=1)
plot(svmfit,dat)
